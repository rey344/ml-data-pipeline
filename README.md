# ML Data Pipeline

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Next.js 14+](https://img.shields.io/badge/Next.js-14+-black.svg)](https://nextjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)

A **production-ready full-stack platform** for training, deploying, and managing machine learning models with an intuitive web interface.

[Features](#features) â€¢ [Quick Start](#quick-start) â€¢ [Architecture](#architecture) â€¢ [API Documentation](#api-documentation) â€¢ [Contributing](#contributing)

</div>

---

## Overview

ML Data Pipeline is an end-to-end machine learning platform designed to streamline the workflow from data ingestion through model deployment. Built with modern technologies, it provides a seamless experience for data scientists and ML engineers to:

- **Upload & explore** datasets (CSV, JSON, Parquet)
- **Preprocess data** with automated cleaning and feature engineering
- **Train models** using various algorithms (Linear Regression, Random Forest, Gradient Boosting, Neural Networks)
- **Evaluate & compare** model performance with interactive visualizations
- **Deploy models** to production with one click
- **Monitor** predictions and model performance in real-time
- **Version control** all models and datasets for reproducibility

---

## Features

### ğŸ¯ Core Capabilities
- âœ… **Multi-format Data Support**: CSV, JSON, Parquet, Excel
- âœ… **Interactive Data Exploration**: Statistical summaries, correlations, distributions
- âœ… **Automated Preprocessing**: Handling missing values, encoding, scaling
- âœ… **Multiple ML Algorithms**: Regression, Classification, Clustering
- âœ… **Hyperparameter Optimization**: Grid search, random search
- âœ… **Model Comparison**: Side-by-side performance metrics
- âœ… **Real-time Predictions**: Batch and single-instance inference
- âœ… **Model Versioning**: Track all model iterations
- âœ… **REST API**: Production-ready endpoints with OpenAPI documentation
- âœ… **Role-based Access Control**: User management and permissions
- âœ… **Experiment Tracking**: Monitor training runs and metrics
- âœ… **Export Functionality**: Download models, predictions, reports

### ğŸ—ï¸ Technical Stack

**Frontend**
- Next.js 14 with React 18
- TypeScript for type safety
- TailwindCSS for styling
- Recharts for data visualization
- SWR for data fetching
- Zustand for state management

**Backend**
- FastAPI (Python 3.10+)
- Pydantic for data validation
- SQLAlchemy ORM
- Alembic for database migrations
- Celery for async job processing

**Data Science & ML**
- scikit-learn for traditional ML
- TensorFlow/PyTorch for deep learning
- Pandas for data manipulation
- NumPy for numerical computing
- Matplotlib/Seaborn for visualization

**Infrastructure**
- PostgreSQL for data storage
- Redis for caching and job queues
- Docker & Docker Compose
- GitHub Actions for CI/CD
- AWS/GCP ready deployment configs

---

## Quick Start

### Prerequisites
- Docker & Docker Compose (recommended)
- Python 3.10+
- Node.js 18+
- PostgreSQL 14+
- Redis 7+

### Installation (Docker - Recommended)

```bash
# Clone the repository
git clone https://github.com/rey344/ml-data-pipeline.git
cd ml-data-pipeline

# Copy environment file
cp .env.example .env

# Start all services
docker-compose up -d

# Initialize database
docker-compose exec api python -m app.commands.init_db

# Access the application
# Frontend: http://localhost:3000
# API Docs: http://localhost:8000/docs
```

### Manual Installation

**Backend Setup**
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
cp .env.example .env

# Run migrations
alembic upgrade head

# Start FastAPI server
uvicorn app.main:app --reload
```

**Frontend Setup**
```bash
cd frontend

# Install dependencies
npm install

# Set environment variables
cp .env.example .env.local

# Start Next.js development server
npm run dev
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Interface (Next.js)                â”‚
â”‚  â”œâ”€ Dashboard: Overview & quick stats                       â”‚
â”‚  â”œâ”€ Data Management: Upload, explore, preprocess            â”‚
â”‚  â”œâ”€ Model Training: Algorithm selection, hyperparameter     â”‚
â”‚  â”œâ”€ Evaluation: Metrics, visualizations, comparisons        â”‚
â”‚  â””â”€ Deployment: Model serving, monitoring                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ REST API (OpenAPI/Swagger)
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API Layer (FastAPI)                       â”‚
â”‚  â”œâ”€ /api/auth: Authentication & authorization               â”‚
â”‚  â”œâ”€ /api/datasets: Data management                          â”‚
â”‚  â”œâ”€ /api/models: Model CRUD operations                      â”‚
â”‚  â”œâ”€ /api/training: Training jobs                            â”‚
â”‚  â”œâ”€ /api/predictions: Inference endpoints                   â”‚
â”‚  â””â”€ /api/monitoring: Performance metrics                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Databaseâ”‚ â”‚  Redis   â”‚ â”‚ Celery  â”‚
   â”‚ (PG)    â”‚ â”‚ (Cache)  â”‚ â”‚ (Jobs)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²            â”‚            â–²
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Model Storageâ”‚         â”‚   Artifacts  â”‚
   â”‚   (S3/GCS)   â”‚         â”‚   (Logs)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
ml-data-pipeline/
â”œâ”€â”€ backend/                    # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/               # API route handlers
â”‚   â”‚   â”œâ”€â”€ models/            # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ ml/                # ML pipeline code
â”‚   â”‚   â”œâ”€â”€ commands/          # CLI commands
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI app
â”‚   â”œâ”€â”€ tests/                 # Unit & integration tests
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/                   # Next.js application
â”‚   â”œâ”€â”€ app/                   # Next.js app directory
â”‚   â”‚   â”œâ”€â”€ (auth)/            # Authentication routes
â”‚   â”‚   â”œâ”€â”€ (dashboard)/       # Dashboard routes
â”‚   â”‚   â”œâ”€â”€ api/               # API routes
â”‚   â”‚   â””â”€â”€ layout.tsx         # Root layout
â”‚   â”œâ”€â”€ components/            # Reusable React components
â”‚   â”œâ”€â”€ hooks/                 # Custom React hooks
â”‚   â”œâ”€â”€ lib/                   # Utilities & helpers
â”‚   â”œâ”€â”€ public/                # Static assets
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml          # Multi-container setup
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/             # CI/CD pipelines
â””â”€â”€ README.md
```

---

## API Documentation

Full API documentation is available at `/docs` endpoint when the backend is running.

### Example Endpoints

```bash
# Upload dataset
POST /api/datasets
Content-Type: multipart/form-data

# List datasets
GET /api/datasets

# Train model
POST /api/models/train
{"dataset_id": "123", "algorithm": "random_forest", "parameters": {...}}

# Get predictions
POST /api/predictions
{"model_id": "456", "data": [[...], [...]]}

# Model metrics
GET /api/models/{model_id}/metrics
```

See [API_DOCUMENTATION.md](./API_DOCUMENTATION.md) for complete details.

---

## Usage Examples

### Training a Model

```python
import requests

# 1. Upload dataset
with open('data.csv', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/api/datasets',
        files={'file': f},
        data={'name': 'iris_dataset'}
    )
    dataset_id = response.json()['id']

# 2. Train model
response = requests.post(
    'http://localhost:8000/api/models/train',
    json={
        'dataset_id': dataset_id,
        'algorithm': 'random_forest',
        'parameters': {
            'n_estimators': 100,
            'max_depth': 10
        },
        'test_split': 0.2
    }
)
model_id = response.json()['id']

# 3. Get predictions
response = requests.post(
    f'http://localhost:8000/api/models/{model_id}/predict',
    json={'data': [[5.1, 3.5, 1.4, 0.2]]}
)
print(response.json())
```

---

## Development

### Running Tests

```bash
# Backend tests
cd backend
pip install pytest pytest-cov
pytest tests/ -v --cov=app

# Frontend tests
cd frontend
npm test -- --coverage
```

### Code Quality

```bash
# Backend
cd backend
flake8 app/
black app/ --check
mypy app/

# Frontend
cd frontend
npm run lint
npm run format
```

### Database Migrations

```bash
# Create migration
alembic revision --autogenerate -m "Add new table"

# Apply migration
alembic upgrade head

# Rollback
alembic downgrade -1
```

---

## Deployment

### Docker Deployment

```bash
# Build images
docker-compose build

# Deploy
docker-compose up -d
```

### Cloud Deployment

- **AWS**: See [deployment/aws/README.md](./deployment/aws/)
- **GCP**: See [deployment/gcp/README.md](./deployment/gcp/)
- **Heroku**: See [deployment/heroku/README.md](./deployment/heroku/)

---

## Performance Metrics

- **Model Training**: <5 minutes for typical datasets (< 100k rows)
- **Prediction Latency**: <100ms per instance
- **Throughput**: 1000+ predictions/second with auto-scaling
- **Data Upload**: Supports files up to 5GB
- **Model Storage**: Versioned storage with deduplication

---

## Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

For detailed contribution guidelines, see [CONTRIBUTING.md](./CONTRIBUTING.md).

---

## Roadmap

- [ ] AutoML capabilities
- [ ] Multi-GPU training support
- [ ] Model explainability (SHAP, LIME)
- [ ] Advanced monitoring & alerting
- [ ] A/B testing framework
- [ ] Integration with MLflow
- [ ] Support for LLMs (fine-tuning, inference)
- [ ] Graph neural networks support
- [ ] Time series forecasting tools
- [ ] Mobile app for predictions

---

## License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

---

## Support

- ğŸ“– [Documentation](./docs/)
- ğŸ› [Report Issues](https://github.com/rey344/ml-data-pipeline/issues)
- ğŸ’¬ [Discussions](https://github.com/rey344/ml-data-pipeline/discussions)
- ğŸ“§ Email: support@example.com

---

## Authors & Contributors

- **rey344** - Initial work - [@rey344](https://github.com/rey344)
See [CONTRIBUTORS.md](./CONTRIBUTORS.md) for all contributors.

---

<div align="center">

**[â¬† back to top](#ml-data-pipeline)**

Made with â¤ï¸ for the data science community

</div>
